{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color cycle analysis\n",
    "\n",
    "The training process takes several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gzip\n",
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sqlite3\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import colorspacious\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, SeparableConv1D, Activation\n",
    "import tensorflow as tf\n",
    "import sklearn.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring reproducibility\n",
    "\n",
    "For reproducibility, a fully-deterministic training procedure is desired. Seeding the PRNG is not enough. As efforts to ensure reproducibility with GPU-based training (`TF_DETERMINISTIC_OPS=1`) were not successful, CPU-based training was used. Although the number of CPU threads did not seem to affect the results, a single thread was still used, since doing so is [recommended to ensure reproducibility](https://github.com/NVIDIA/framework-determinism/blob/28091e4fb1483685fc78b7ab844a5ae6ddf55a14/README.md#cpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force CPU operation for deterministic and reproducible results\n",
    "# Disable parallelism per https://github.com/NVIDIA/framework-determinism/blob/28091e4fb1483685fc78b7ab844a5ae6ddf55a14/README.md#cpu\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for deterministic results\n",
    "SEED = 567687\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.9 (default, Jan 26 2021, 15:33:00) \n",
      "[GCC 8.4.0]\n",
      "NumPy 1.19.5\n",
      "TensorFlow 2.4.1\n",
      "Scikit-learn 0.24.2\n",
      "Colorspacious 1.1.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Python\", sys.version)\n",
    "print(\"NumPy\", np.__version__)\n",
    "print(\"TensorFlow\", tf.__version__)\n",
    "print(\"Scikit-learn\", sklearn.__version__)\n",
    "print(\"Colorspacious\", colorspacious.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE = \"../survey-results/results.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_NUM_COLORS = [6, 8]\n",
    "DATA_SPLIT_FRAC = 0.8\n",
    "ENSEMBLE_COUNT = 100\n",
    "NUM_EPOCHS = 120\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color conversion and sorting functions\n",
    "\n",
    "Data are all stored as 8-bit RGB values and need to be converted to CAM02-UCS.\n",
    "\n",
    "We also need to be able to sort along the three CAM02-UCS axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_jab(color):\n",
    "    \"\"\"\n",
    "    Convert hex color code (without `#`) to CAM02-UCS.\n",
    "    \"\"\"\n",
    "    rgb = [(int(i[:2], 16), int(i[2:4], 16), int(i[4:], 16)) for i in color]\n",
    "    jab = [colorspacious.cspace_convert(i, \"sRGB255\", \"CAM02-UCS\") for i in rgb]\n",
    "    return np.array(jab)\n",
    "\n",
    "\n",
    "def sort_colors_by_j(colors):\n",
    "    \"\"\"\n",
    "    Sorts colors by CAM02-UCS J' axis.\n",
    "    \"\"\"\n",
    "    return colors[np.lexsort(colors[:, ::-1].T, 0)]\n",
    "\n",
    "\n",
    "def sort_colors_by_a(colors):\n",
    "    \"\"\"\n",
    "    Sorts colors by CAM02-UCS a' axis.\n",
    "    \"\"\"\n",
    "    return colors[np.argsort(colors[:, ::-1].T[1])]\n",
    "\n",
    "\n",
    "def sort_colors_by_b(colors):\n",
    "    \"\"\"\n",
    "    Sorts colors by CAM02-UCS b' axis.\n",
    "    \"\"\"\n",
    "    return colors[np.argsort(colors[:, ::-1].T[2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Survey data are loaded from a SQLite database. The 8-bit RGB values are converted to CAM02-UCS and sorted along the three CAM02-UCS axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 10347\n",
      "8 10371\n"
     ]
    }
   ],
   "source": [
    "# Load survey data\n",
    "cycle_data = {}\n",
    "cycle_targets = {}\n",
    "min_count = 1e10\n",
    "\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "c = conn.cursor()\n",
    "\n",
    "for num_colors in ALL_NUM_COLORS:\n",
    "    count = 0\n",
    "    cycle_data[num_colors] = []\n",
    "    cycle_targets[num_colors] = []\n",
    "    for row in c.execute(\n",
    "        f\"SELECT c1, c2, o, cp, sp FROM picks WHERE length(c1) = {num_colors * 7 - 1}\"\n",
    "    ):\n",
    "        count += 1\n",
    "        orders = [[int(c) for c in o] for o in row[2].split(\",\")]\n",
    "        # Convert to Jab [CAM02-UCS based]\n",
    "        jab1 = to_jab(row[0].split(\",\"))\n",
    "        jab2 = to_jab(row[1].split(\",\"))\n",
    "        # Add cycle data\n",
    "        for i in range(4):\n",
    "            if i != row[3] - 1:\n",
    "                cycle_data[num_colors].append(\n",
    "                    np.array((jab1[orders[row[3] - 1]], jab2[orders[i]])).flatten()\n",
    "                )\n",
    "                cycle_targets[num_colors].append(0)\n",
    "    cycle_data[num_colors] = np.array(cycle_data[num_colors])\n",
    "    cycle_targets[num_colors] = np.array(cycle_targets[num_colors])\n",
    "    min_count = min(min_count, count)\n",
    "    print(num_colors, count)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct, train, and evaluate ensemble instance\n",
    "\n",
    "### Construct network\n",
    "\n",
    "A fully-convolutional architecture is used, and weights are shared between different color cycle sizes. An homogenous ensemble of `ENSEMBLE_COUNT` models is constructed, to be trained in parallel using bootstrap aggregation (bagging) to divide the data into training and test sets.\n",
    "\n",
    "### Split data into training and test sets\n",
    "\n",
    "Bootstrap aggregation (bagging) is used to split the data into training and test sets, with a different data split used for each member of the ensemble. The `DATA_SPLIT_FRAC` sets the average fraction of data used in the training set (this is a statistical property, due to the stocastic nature of bagging). For each data split, all examples not in the training data set are placed in the test data set (which is not always the same size, again due to the stocastic nature of bagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iteration():\n",
    "    #\n",
    "    # Construct network\n",
    "    #\n",
    "    input_a = {}\n",
    "    input_b = {}\n",
    "    model = {}\n",
    "    predictions = {}\n",
    "    scoring_model = {}\n",
    "\n",
    "    reg = tf.keras.regularizers.l2(0.0001)\n",
    "\n",
    "    conv_size = 5\n",
    "\n",
    "    layer1 = Dense(units=5, activation=\"elu\", activity_regularizer=reg, name=\"l1\")\n",
    "    layer2 = Dense(units=5, activation=\"elu\", activity_regularizer=reg, name=\"l2\")\n",
    "\n",
    "    layer3 = SeparableConv1D(\n",
    "        5,\n",
    "        conv_size,\n",
    "        padding=\"same\",\n",
    "        activation=\"elu\",\n",
    "        activity_regularizer=reg,\n",
    "        name=\"l3\",\n",
    "    )\n",
    "    layer4 = SeparableConv1D(\n",
    "        3,\n",
    "        conv_size,\n",
    "        padding=\"same\",\n",
    "        activation=\"elu\",\n",
    "        activity_regularizer=reg,\n",
    "        name=\"l4\",\n",
    "    )\n",
    "    layer5 = SeparableConv1D(\n",
    "        1,\n",
    "        conv_size,\n",
    "        padding=\"same\",\n",
    "        activation=\"elu\",\n",
    "        activity_regularizer=reg,\n",
    "        name=\"l5\",\n",
    "    )\n",
    "\n",
    "    for num_colors in ALL_NUM_COLORS:\n",
    "        # Create network\n",
    "        # One input per color set\n",
    "        input_a[num_colors] = Input(shape=(3 * num_colors,))\n",
    "        input_b[num_colors] = Input(shape=(3 * num_colors,))\n",
    "        inputs_a = [\n",
    "            input_a[num_colors][:, i * 3 : (i + 1) * 3] for i in range(num_colors)\n",
    "        ]\n",
    "        inputs_b = [\n",
    "            input_b[num_colors][:, i * 3 : (i + 1) * 3] for i in range(num_colors)\n",
    "        ]\n",
    "\n",
    "        # Share layers between colors\n",
    "        x_a = [layer1(i / 100) for i in inputs_a]\n",
    "        x_b = [layer1(i / 100) for i in inputs_b]\n",
    "\n",
    "        x_a = [layer2(i) for i in x_a]\n",
    "        x_b = [layer2(i) for i in x_b]\n",
    "\n",
    "        # Combine colors into sets\n",
    "        x_a = tf.keras.layers.concatenate(\n",
    "            [tf.keras.backend.expand_dims(i, 1) for i in x_a], axis=1\n",
    "        )\n",
    "        x_b = tf.keras.layers.concatenate(\n",
    "            [tf.keras.backend.expand_dims(i, 1) for i in x_b], axis=1\n",
    "        )\n",
    "\n",
    "        # Share layers between color sets\n",
    "        x_a = layer3(x_a)\n",
    "        x_b = layer3(x_b)\n",
    "\n",
    "        x_a = layer4(x_a)\n",
    "        x_b = layer4(x_b)\n",
    "\n",
    "        x_a = layer5(x_a)\n",
    "        x_b = layer5(x_b)\n",
    "\n",
    "        # Average outputs\n",
    "        x_a = tf.math.reduce_mean(x_a, axis=1)\n",
    "        x_b = tf.math.reduce_mean(x_b, axis=1)\n",
    "\n",
    "        # Make conjoined network with output score\n",
    "        layer_nm1 = Activation(\"sigmoid\", name=f\"score{num_colors}\")\n",
    "        x_a = layer_nm1(x_a)\n",
    "        x_b = layer_nm1(x_b)\n",
    "        predictions[num_colors] = tf.keras.layers.subtract([x_b, x_a])\n",
    "        predictions[num_colors] = Activation(\"sigmoid\", name=f\"a{num_colors}\")(\n",
    "            predictions[num_colors]\n",
    "        )\n",
    "\n",
    "        # Compile model\n",
    "        model[num_colors] = Model(\n",
    "            inputs=[input_a[num_colors], input_b[num_colors]],\n",
    "            outputs=predictions[num_colors],\n",
    "        )\n",
    "        model[num_colors].compile(loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "        # For evaluation\n",
    "        scoring_model[num_colors] = Model(inputs=input_a[num_colors], outputs=x_a)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01, centered=True)\n",
    "    modelc = Model(\n",
    "        inputs=[i[nc] for nc in ALL_NUM_COLORS for i in (input_a, input_b)],\n",
    "        outputs=[predictions[nc] for nc in ALL_NUM_COLORS],\n",
    "    )\n",
    "    modelc.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"acc\"])\n",
    "\n",
    "    #\n",
    "    # Create training / test data splits\n",
    "    #\n",
    "    x_trains = {}\n",
    "    y_trains = {}\n",
    "    x_tests = {}\n",
    "    y_tests = {}\n",
    "\n",
    "    for num_colors in ALL_NUM_COLORS:\n",
    "        # Split dataset (bagging)\n",
    "        # The three pairs from each response are kept together\n",
    "        idx = np.arange(min_count)\n",
    "        train_idx = sklearn.utils.resample(\n",
    "            idx,\n",
    "            replace=True,\n",
    "            n_samples=int(min_count * DATA_SPLIT_FRAC / (1 - 1 / np.e)),\n",
    "        )\n",
    "        test_idx = np.array([i for i in idx if i not in train_idx])\n",
    "        train_idx = np.concatenate([train_idx * 3, train_idx * 3 + 1, train_idx * 3 + 2])\n",
    "        test_idx = np.concatenate([test_idx * 3, test_idx * 3 + 1, test_idx * 3 + 2])\n",
    "        x_trains[num_colors] = cycle_data[num_colors][train_idx]\n",
    "        y_trains[num_colors] = cycle_targets[num_colors][train_idx]\n",
    "        x_tests[num_colors] = cycle_data[num_colors][test_idx]\n",
    "        y_tests[num_colors] = cycle_targets[num_colors][test_idx]\n",
    "\n",
    "    #\n",
    "    # Fit network\n",
    "    #\n",
    "    h = modelc.fit(\n",
    "        list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    [\n",
    "                        x_trains[nc][:, : nc * 3],\n",
    "                        x_trains[nc][:, nc * 3 :],\n",
    "                    ]\n",
    "                    for nc in ALL_NUM_COLORS\n",
    "                ]\n",
    "            )\n",
    "        ),\n",
    "        [y_trains[nc] for nc in ALL_NUM_COLORS],\n",
    "        epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Evaluate network\n",
    "    #\n",
    "    eval_train = {}\n",
    "    eval_test = {}\n",
    "    for nc in ALL_NUM_COLORS:\n",
    "        eval_train[nc] = model[nc].evaluate(\n",
    "            [x_trains[nc][:, : nc * 3], x_trains[nc][:, nc * 3 :]],\n",
    "            y_trains[nc],\n",
    "            batch_size=8192,\n",
    "            verbose=0,\n",
    "        )[1]\n",
    "        eval_test[nc] = model[nc].evaluate(\n",
    "            [x_tests[nc][:, : nc * 3], x_tests[nc][:, nc * 3 :]],\n",
    "            y_tests[nc],\n",
    "            batch_size=8192,\n",
    "            verbose=0,\n",
    "        )[1]\n",
    "\n",
    "    return eval_train, eval_test, scoring_model[ALL_NUM_COLORS[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ensemble\n",
    "\n",
    "Iteratively train each instance of model ensemble. The train and test set accuracy are recorded, and the trained model weights are saved.\n",
    "\n",
    "### Save model weights\n",
    "\n",
    "Model weights are saved in the HDF5 format, to allow for loading by name. This is important, since it allows for loading the weights into a model with a different topology, e.g., a model that only works for a different color set size. While the model weights are independent of color set size, the actual model needs to be constructed with a fixed number of colors (at least using TensorFlow). Gzip compression is used on the HDF5 files, since this reduces their sizes by more than an order of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters per ensemble instance: 167\n",
      "Weight sum: 9349.75452\n",
      "Training time: 22182.6s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "os.makedirs(\"weights\", exist_ok=True)\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "weight_sum = 0\n",
    "\n",
    "# Without this, TensorFlow will start complaining about retracing after the first couple iterations\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "for i in range(ENSEMBLE_COUNT):\n",
    "    eval_train, eval_test, scoring_model = run_iteration()\n",
    "    train_acc.append(eval_train)\n",
    "    test_acc.append(eval_test)\n",
    "\n",
    "    # Sum weights to check for reproducibility\n",
    "    weight_sum += sum([np.sum(np.abs(w)) for w in scoring_model.get_weights()])\n",
    "\n",
    "    # Save weights\n",
    "    weight_file_name = f\"weights/cycle_model_weights_{i:03d}.h5\"\n",
    "    scoring_model.save_weights(weight_file_name)\n",
    "    with open(weight_file_name, \"rb\") as infile:\n",
    "        with gzip.open(weight_file_name + \".gz\", \"wb\") as outfile:\n",
    "            shutil.copyfileobj(infile, outfile)\n",
    "    os.remove(weight_file_name)\n",
    "\n",
    "    if i == 0:\n",
    "        param_count = scoring_model.count_params()\n",
    "        print(\"Number of parameters per ensemble instance:\", param_count)\n",
    "\n",
    "print(f\"Weight sum: {weight_sum:.5f}\")\n",
    "print(f\"Training time: {time.time() - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 6: 0.54394 +/- 0.00418\n",
      " test accuracy 6: 0.52585 +/- 0.00649\n",
      "train accuracy 8: 0.54638 +/- 0.00483\n",
      " test accuracy 8: 0.52977 +/- 0.00582\n"
     ]
    }
   ],
   "source": [
    "for nc in ALL_NUM_COLORS:\n",
    "    acc = [i[nc] for i in train_acc]\n",
    "    print(f\"train accuracy {nc}: {np.mean(acc):.5f} +/- {np.std(acc):.5f}\")\n",
    "    acc = [i[nc] for i in test_acc]\n",
    "    print(f\" test accuracy {nc}: {np.mean(acc):.5f} +/- {np.std(acc):.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
